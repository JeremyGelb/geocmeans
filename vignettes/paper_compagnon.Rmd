---
title: "Spatial Fuzzy CMean with R, paper helper"
author: "Jeremy Gelb"
date: "13/04/2020"
output:
  rmarkdown::html_vignette:
    fig_width: 5
    fig_height: 5
    toc: true
    toc_depth: 2
    df_print: "tibble"
vignette: >
  %\VignetteIndexEntry{paper_helper}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This vignette is a companion for the paper : "Add the reference to the paper when ready". The main goal is to present the FCM_S (spatially constrained c-means) and to compare the results with a classical c-means, a HAC, and a spatialy constrained HAC. 


# Preparing environment and dataset

We start here with loading the main packages, dataset and standardization of the variables prior to the clustering.

```{r message=FALSE, warning=FALSE}
library(ClustGeo)
library(geocmeans)
library(ggplot2)
library(RColorBrewer)
library(ggpubr)
library(spdep)
library(kableExtra)
data("LyonIris")


#selection of the columns used in the analysis
Vars <- c("Lden","NO2", "PM25", "VegHautPrt", "Pct0_14", "Pct_65", "Pct_Img",
          "TxChom1564", "Pct_brevet", "NivVieMed")

#standardization of the variables
for(v in Vars){
  LyonIris[[paste("std_",v,sep="")]] <- scale(LyonIris[[v]])
}

#selection of the standardized columns
stdVars <- names(LyonIris)[grepl("std_",names(LyonIris),fixed = T)]

#setting the random parameter
myseed <- 125899657
set.seed(myseed)

#building a neighbouring matrix
nb <- poly2nb(LyonIris)
listw <- nb2listw(nb,style = "W")

```

# Classical c-means

The starting point will be the classical c-means classification algorithm. We will use it to find for our dataset the best values for k (the number of groups) and m (the fuzziness parameter). This can be done easily with the function *select_parameters* from the geocmeans package.

```{r message=FALSE, warning=FALSE}
data <- LyonIris@data[stdVars]

#lets do this with 4 cores
future::plan(future::multiprocess(workers=4))
params_df <- select_parameters(data,k = 2:9, m = seq(1.1,3,0.1),
                  alpha = 0, nblistw=listw,standardize = F, tol=0.001, seed = myseed, spconsist = F)

#back to single process
if (!inherits(future::plan(), "sequential")) future::plan(future::sequential)
```

To find the best parameters, we can plot the different quality indices.

```{r fig.width=10}
params_df$k <- as.character(params_df$k)
colors <- brewer.pal(8, "Accent")

P1 <- ggplot(params_df)+
  geom_smooth(aes(x=m,y=Explained.inertia, color=k, group=k))+
  geom_point(aes(x=m,y=Explained.inertia,color=k))+
  scale_colour_manual(values=colors)

P2 <- ggplot(params_df)+
  geom_smooth(aes(x=m,y=Partition.coeff, color=k, group=k))+
  geom_point(aes(x=m,y=Partition.coeff,color=k))+
  scale_colour_manual(values=colors)

P3 <- ggplot(params_df)+
  geom_smooth(aes(x=m,y=Silhouette.index, color=k, group=k))+
  geom_point(aes(x=m,y=Silhouette.index,color=k))+
  scale_colour_manual(values=colors)

P4 <- ggplot(params_df)+
  geom_smooth(aes(x=m,y=FukuyamaSugeno.index, color=k, group=k))+
  geom_point(aes(x=m,y=FukuyamaSugeno.index,color=k))+
  scale_colour_manual(values=colors)

P5 <- ggplot(params_df)+
  geom_smooth(aes(x=m,y=XieBeni.index, color=k, group=k),se=F)+
  geom_point(aes(x=m,y=XieBeni.index,color=k))+
  scale_colour_manual(values=colors)+ylim(0,5)


print(ggarrange(P1,P2,P3,P4,P5, ncol = 2, nrow=3))

```

Considering the previous plots, we define k = 4, and m = 1.5. All the quality indices are lower for higher values of m, and the three first groups seem to explain most of the inertia. We decided to use k=4 instead of k=3 to potentially reveal a less pronounced group without reducing too much the silhouette index.

```{r}
cmean_result <- CMeans(data, 4 , 1.5, tol=0.001, seed = myseed, verbose=F, standardize = F)

```

# Classical HAC

Now that k is defined, we can realize the HAC analysis
```{r}

## generating the dissimilarity matrix
n <- nrow(data)
Do <- dist(data)

##calssical HAC using the Ward method
tree <- hclustgeo(Do)

LyonIris$class_HAC <- cutree(tree,4)

df <- data.frame("V"=as.factor(LyonIris$class_HAC))
HAC_belong <- model.matrix(~ . + 0, data=df, contrasts.arg = lapply(df, contrasts, contrasts=FALSE))

```


# Spatialy constrained HAC

The package **ClustGeo** provide a ward-like method to include geographical distance between observations in the classical HAC. This add another parameter to find in the analysis : $\alpha$. It controls the weight of the spatial distance matrix in the analysis.

```{r}
D0 <- dist(data)
D1 <- dist(coordinates(LyonIris))
range.alpha <- seq(0,1,0.05)

cr <- choicealpha(D0,D1,range.alpha,4,graph=TRUE)
```

We will retain a value of 0.35 for $\alpha$. This value achieve the best deal between spatial and semantical inertia. The first drops by only 2% when the second increases by more than 25%.

```{r}
tree <- hclustgeo(D0,D1,alpha=0.35)
LyonIris$class_spHAC <- cutree(tree,4)

df <- data.frame("V"=as.factor(LyonIris$class_spHAC))
spHAC_belong <- model.matrix(~ . + 0, data=df, contrasts.arg = lapply(df, contrasts, contrasts=FALSE))
```

# spatially constrained c-means (SFCM)

for the SFCM, we  have to select a parameter ($\alpha$) controlling the weight of the spatially lagged dataset.

```{r message=FALSE, warning=FALSE}
listw2 <- adjustSpatialWeights(data,listw$neighbours,style="C")

values <- select_parameters(data, k = 4, m = 1.5, alpha = seq(0,2,0.05),
            nblistw = listw, standardize = F, tol = 0.001, seed = myseed)
```

```{r fig.height=8, fig.width=8}

P1 <- ggplot(values)+
  geom_smooth(aes(x=alpha,y=Explained.inertia), color="black")+
  geom_point(aes(x=alpha,y=Explained.inertia), color="red")

P2 <- ggplot(values)+
  geom_smooth(aes(x=alpha,y=Partition.coeff), color="black")+
  geom_point(aes(x=alpha,y=Partition.coeff), color="red")

P3 <- ggplot(values)+
  geom_smooth(aes(x=alpha,y=Silhouette.index), color="black")+
  geom_point(aes(x=alpha,y=Silhouette.index), color="red")

P4 <- ggplot(values)+
  geom_smooth(aes(x=alpha,y=FukuyamaSugeno.index), color="black")+
  geom_point(aes(x=alpha,y=FukuyamaSugeno.index), color="red")

P5 <- ggplot(values)+
  geom_smooth(aes(x=alpha,y=XieBeni.index),se=F, color="black")+
  geom_point(aes(x=alpha,y=XieBeni.index), color="red")

P6 <- ggplot(values)+
  geom_ribbon(aes(ymin = spConsistency_05, ymax = spConsistency_95, x=alpha))+
  geom_smooth(aes(x=alpha,y=spConsistency),se=F, color="black")+
  geom_point(aes(x=alpha,y=spConsistency), color="red")



print(ggarrange(P1,P2,P3,P4,P5,P6, ncol = 2, nrow=3))

```

Without any surprise, the explained inertia is following a decreasing trend when alpha increases. But it is interesting to note that for the partition coefficient and the partition entropy, a maximum is obtained when alpha = 0.5. This maximum provides a better classification than the classical cmeans (when alpha=0). For the silhouette index it seems that increasing alpha enhance the quality of the classification until alpha = 0.7. We decide to select alpha = 0.7


```{r}
SFCM_result <- SFCMeans(data, listw, k=4, m=1.5, alpha=0.7,
                        tol = 0.0001, standardize = F, seed=myseed)

```

# comparing results of all algorithm

It is now time to compare the results of all algorithm. We will start with a purely semantic point of view, then analyze the spatial dimension.

## comparing adjustment indices

```{r}

HACQual <- calcqualityIndexes(data, HAC_belong, m=1)
CmeansQual <- calcqualityIndexes(data, cmean_result$Belongings, m=1.5)
spHACQual <- calcqualityIndexes(data,spHAC_belong, m=1)
SFCMQual <- calcqualityIndexes(data,SFCM_result$Belongings, m=1.5)

comp <- as.data.frame(t(do.call(rbind,lapply(list(HACQual,CmeansQual,spHACQual,SFCMQual),as.numeric))))
colnames(comp) <- c("HAC","Cmeans","spHAC","spCmeans")
rownames(comp) <- names(HACQual)
kable(comp,digits = 3)
```
At that point, it is interesting to note three things : 

1. The hard clustering methods obtain a better explained inertia
2. The soft clustering methods obtain a better fuzzy silhouette index
3. The loss in therms of inertia in small when using a spatially constrained method


## Analyzing spatial dimension of the classifications

The function *spatialDiag* of **geocmeans** gives a lot of information about the spatial dimension of the classification result

```{r}

HACspDiag <- spatialDiag(HAC_belong, listw, nrep = 999)
CmeansspDiag  <- spatialDiag(cmean_result$Belongings, listw, nrep = 999)
spHACspDiag  <- spatialDiag(spHAC_belong, listw, nrep = 999)
SFCMspDiag  <- spatialDiag(SFCM_result$Belongings, listw, nrep = 999)

```

We can start with the simple Moran Values calculated on the belonging matrices

```{r}
diaglist <- list(HACspDiag,CmeansspDiag,spHACspDiag,SFCMspDiag)

moranTable <- do.call(rbind,lapply(diaglist, function(x){
  return(as.numeric(x$MoranValues$MoranI))
}))
rownames(moranTable) <- c("HAC","Cmeans","spHAC","spCmeans")
kable(moranTable,digits = 3)
```

This results show that the fuzzy classification is more able to produce spatially autocorrelated groups than the hard clustering. Without any surprise, the spatially constrained methods achieve higher Moran I on the columns of their belonging matrices for all the groups.

It seems that one group is significantly more spatially fragmented than the others if we look at the Moran Values of the two spatially constrained classifications. This might be explained by the presence of observation laying right between two clusters (undecided units).

To complete the diagnostic, we could calculate another version of the spatial-consistency which takes into account the semantic distance between observations.
```{r}
HACspDiag2 <- spatialDiag(HAC_belong, listw2, nrep = 999)
CmeansspDiag2  <- spatialDiag(cmean_result$Belongings, listw2, nrep = 999)
spHACspDiag2  <- spatialDiag(spHAC_belong, listw2, nrep = 999)
SFCMspDiag2  <- spatialDiag(SFCM_result$Belongings, listw2, nrep = 999)
diaglist2 <- list(HACspDiag2,CmeansspDiag2,spHACspDiag2,SFCMspDiag2)
```

We can now compare the degree of spatial consistency of the different classifications

```{r}
spconsist <- do.call(rbind,lapply(diaglist,function(x){
  c(round(x$SpConsist,3),
    round(quantile(x$SpConsistSamples,probs =0.05),3),
    round(quantile(x$SpConsistSamples,probs =0.95),3))
  }))
spconsist2 <- do.call(rbind,lapply(diaglist2,function(x){
  c(round(x$SpConsist,3),
    round(quantile(x$SpConsistSamples,probs =0.05),3),
    round(quantile(x$SpConsistSamples,probs =0.95),3))
  }))
df <- data.frame(t(cbind(spconsist,spconsist2)))
colnames(df) <- c("HAC","FCM","ClustGeo","SFCM")
rownames(df) <- c("mean Scr",
                  "0.05 Scr",
                  "0.95 Scr",
                  "mean Smcr",
                  "0.05 Smcr",
                  "0.95 Smcr"
                  )
kable(df,digits = 3)

```

The spatially constrained cmeans achieves the better spatial consistency without reducing that much the quality of the clustering.

Finally, we can map the clusters to compare graphically the results.

```{r message=FALSE, warning=FALSE}
HAC_maps <- mapClusters(LyonIris,belongmatrix = HAC_belong)
Cmeans_maps <- mapClusters(LyonIris,belongmatrix = cmean_result$Belongings)
spHAC_maps <- mapClusters(LyonIris,belongmatrix = spHAC_belong)
spCmeans_maps <- mapClusters(LyonIris,belongmatrix = SFCM_result$Belongings)
```

The point is to find which cluster is similar to which one in each classification. We do this here by hand

```{r fig.height=10, fig.width=10, message=FALSE, warning=FALSE}
allmaps <- list(HAC_maps,Cmeans_maps,spHAC_maps,spCmeans_maps)

map_order <-rbind(c(1,3,2,4),
                  c(2,1,4,3),
                  c(1,3,4,2),
                  c(2,1,4,3))

algorithm <- c("HAC","FCM","ClustGeo","SFCM")

map_list <- lapply(1:4,function(i){
  ids <- map_order[,i]
  maps <- lapply(1:length(ids),function(m){
    map <- allmaps[[m]]$ProbaMaps[ids[[m]]][[1]]
    map <- map+ theme(legend.position = "none",
          panel.background = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          plot.margin = unit(c(0.05,0.05,0.05,0.05), "lines"))
    if(i==1){
      map <- map+ggtitle(algorithm[[m]])+theme(plot.title = element_text(hjust = 0.5))
    }
    return(map)
  })
  return(maps)
})
all_probamaps <- unlist(map_list,recursive = F)

## adding now the final class maps
colors <- c("#6eb97e","#386cb0","#fdc086","#ffff99")
for(i in 1:4){
  actual_map <- allmaps[[i]]
  index <- map_order[i,]
  names(colors) <- paste("V",index,sep="")
  clustmap <- actual_map$ClusterPlot + 
    scale_fill_manual(values = colors)+
    theme(legend.position = "none",
          panel.background = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          plot.margin = unit(c(0.05,0.05,0.05,0.05), "lines"))
  all_probamaps[[length(all_probamaps)+1]] <- clustmap
}

print(ggarrange(plotlist = all_probamaps, ncol=4,nrow=5))

```

All the groups are very similar for each classification but the last one. The fuzzy classifications sho more uncertainty for this group which is an interesting property.

## Describe groups

finally we will compute some descriptive statistics to analyze the groups produced by the spatially constrained cmeans algorithm.

```{r}

spCmeansSummary <- summarizeClusters(LyonIris@data[Vars],belongmatrix = SFCM_result$Belongings, weighted = T, dec = 2)

kable(spCmeansSummary$Cluster_1, caption = "descriptive values of group 1")
kable(spCmeansSummary$Cluster_2, caption = "descriptive values of group 2")
kable(spCmeansSummary$Cluster_3, caption = "descriptive values of group 3")
kable(spCmeansSummary$Cluster_4, caption = "descriptive values of group 4")
```

To help the overall distinction between clusters, we can also use some plots like violin plots and spider plots
```{r}
sfcm_belong_mat <-  SFCM_result$Belongings[,c(2,1,4,3)]
cmeans_belong_mat <- cmean_result$Belongings[,c(1,3,2,4)]
spiderPlots(LyonIris@data[Vars], sfcm_belong_mat, chartcolors = c("darkorange3","grey4","gray49","royalblue"))
```

```{r fig.height=15, fig.width=5}
violin_plots <- violinPlots(LyonIris@data[Vars], SFCM_result$Groups)

ggarrange(plotlist = violin_plots, ncol = 2, nrow=6)
```

We should count the number of observation in each cluster too.
```{r}
belong_matrices <- list(HAC_belong, cmean_result$Belongings, spHAC_belong ,SFCM_result$Belongings)

counts <- lapply(1:4,function(i){
  mat <- belong_matrices[[i]]
  idx <- map_order[i,]
  maxs <- apply(mat,1,max)
  counts <- apply(mat,2,function(column){
    return(sum(column == maxs))
  })
  return(counts[idx])
})

counts <- do.call(rbind,counts)
row.names(counts) <- c("HAC","FCM","ClustGeo","SFCM")
kable(counts)

```
We could also ensure that the clusters identified by the 4 methods are not to different
```{r}

all_summary <- lapply(belong_matrices, function(x){
  return(summarizeClusters(LyonIris@data[Vars],belongmatrix = x, weighted = T, dec = 2))
  })

all_table <- lapply(1:4,function(i){
  ids <- map_order[,i]
  cols <- lapply(1:length(all_summary),function(j){
    x <- all_summary[[j]]
    return(as.numeric(x[[ids[[j]]]][8,]))
  })
  clusttable <- do.call(cbind,cols)
  rownames(clusttable) <- Vars
  colnames(clusttable) <- algorithm
  return(clusttable)
})

completetable <- do.call(cbind,all_table)

kable(completetable) %>%
  add_header_above(c(" "=1,"Group 1" = 4, "Group 2" = 4, "Group 3" = 4, "Group 4" = 4))
```

## Uncertain observations

We will extract here for the SFCM algorithm the observations for which the classification is difficult. We will first observe the distribution of the most likely cluster for each observation

```{r}
## first, we can extract the greatest prob of belonging for each observation
most_likely <- apply(sfcm_belong_mat,1,max)
hist(most_likely,breaks=30)
quantile(most_likely,probs = c(0.05,0.1,0.25,0.5,0.75,0.9,0.95))

```
Considering the previous distribution, we decide to label as unclassified the observations for which the maximum probability in the belonging matrix is below 0.4.



```{r}
classes <- undecidedUnits(sfcm_belong_mat,tol = 0.4)
Allmaps <- mapClusters(LyonIris,sfcm_belong_mat, undecided = 0.4)

LyonIris$class_SFCM <- classes
sfcm_belong_mat <- data.frame(sfcm_belong_mat)
names(sfcm_belong_mat) <- paste("groupe_",1:ncol(sfcm_belong_mat),sep="")
for(i in names(sfcm_belong_mat)){
  LyonIris[[i]] <- sfcm_belong_mat[[i]]
}
#rgdal::writeOGR(LyonIris,"sfcm_results.gpkg",driver="GPKG",layer="sfcm")
```

## adjusting for the spatial parameters

We still have to choose between different types of matrices and between the two methods for the spatial neighbouring.
We can try to adjust all these parameters simultaneously with alpha and compare results.
```{r}

## classical Queen matrix
Queen <- poly2nb(LyonIris)
WQueen <- nb2listw(Queen)

## classical Rook matrix
Rook <- poly2nb(LyonIris, queen = F)
WRook <- nb2listw(Rook)

## nearest neighbours matrices
coords <- coordinates(LyonIris)
nneighbours <- lapply(5:10,function(i){
  knn <- knearneigh(coords, k=i)
  nb <- knn2nb(knn)
  wnb <- nb2listw(nb)
  return(wnb)
})

allWlist <- nneighbours
allWlist[[length(allWlist)+1]] <- WRook
allWlist[[length(allWlist)+1]] <- WQueen

future::plan(future::multiprocess(workers=6))
values <- select_parameters.mc(data, k = 4, m = 1.5, alpha = seq(0,2,0.05),
            nblistw = allWlist, lag_method = c("mean","median"), standardize = F, tol = 0.001, seed = myseed)

#back to single process
if (!inherits(future::plan(), "sequential")) future::plan(future::sequential)

listwnames <- c("queen","rook",paste("nearest neighbour ",5:10, sep=""))

inertia_plots <- lapply(1:length(listwnames),function(i){
  sub <- subset(values,values$listw==i)
  tmpplot <- ggplot(data=sub)+
    geom_line(aes(x=alpha,y=Explained.inertia,color=lag_method))+
    geom_point(aes(x=alpha,y=Explained.inertia,color=lag_method))+
    ggtitle(listwnames[[i]])
})

spConsistency_plots <- lapply(1:length(listwnames),function(i){
  sub <- subset(values,values$listw==i)
  tmpplot <- ggplot(data=sub)+
    geom_line(aes(x=alpha,y=spConsistency,color=lag_method))+
    geom_point(aes(x=alpha,y=spConsistency,color=lag_method))+
    ggtitle(listwnames[[i]])
})


ggarrange(plotlist = inertia_plots, ncol=2, nrow = 4)

ggarrange(plotlist = spConsistency_plots, ncol=2, nrow = 4)

```

We can see that the choice of the spatial matrix does not affect too much the results, but the median method seems to perform worst.
